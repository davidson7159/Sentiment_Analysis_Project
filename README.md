# ğŸ“± SMS Spam Classification (NLP Project)

A complete end-to-end machine learning project to classify SMS messages as **Spam** or **Ham** using NLP techniques and classic ML models.

This project includes:
- Exploratory Data Analysis (EDA)
- Text preprocessing
- Feature extraction (TF-IDF)
- Model training and evaluation
- Model selection
- Export of final model + vectorizer
- CLI prediction script (`predict.py`)

---

## ğŸš€ Project Structure
sms-spam-nlp/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # raw dataset
â”‚   â””â”€â”€ processed/           # cleaned dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA_SMS_Spam.ipynb
â”‚   â”œâ”€â”€ 02_Preprocessing.ipynb
â”‚   â””â”€â”€ 03_Model_Selection_SMS.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pkl       # saved model
â”‚   â””â”€â”€ tfidf_vectorizer.pkl # saved TF-IDF vectorizer
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ predict.py           # CLI prediction script
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ“Š Dataset

The dataset used is the **SMS Spam Collection Dataset**, containing  
> 5,574 SMS messages labeled as `spam` or `ham`.

Source: Kaggle.

Columns used:
- **v1** â†’ label (`ham` / `spam`)
- **v2** â†’ message text

---

## ğŸ” Exploratory Data Analysis

Key findings:
- Only **13%** of messages are spam â†’ imbalanced dataset  
- Spam messages are generally **longer** than ham messages  
- Frequent spam keywords include: *free, call, now, claim, prize*  
- Frequent ham keywords include: *ok, tomorrow, home, thanks*

Visualizations include:
- Label distribution  
- Message length distribution  
- Most common spam vs ham words  

All results are in `01_EDA_SMS_Spam.ipynb`.

---

## ğŸ§¹ Text Preprocessing

Steps applied:
- Lowercasing  
- Removing punctuation  
- Removing stopwords  
- Tokenization  
- Lemmatization (optional)  
- Joining tokens back into processed text  

Final cleaned dataset saved as:
data/processed/sms_preprocessed.csv
Notebook: `02_Preprocessing.ipynb`

---

## ğŸ§  Modeling

Models trained:
- Logistic Regression  
- Multinomial Naive Bayes  
- Random Forest
- XGBoost  

Feature extraction:  
TfidfVectorizer(ngram_range=(1,2))

Evaluation metric:
- **F1-score** (important due to class imbalance)

Notebook: `03_Modeling_SMS_Spam.ipynb`

---

## ğŸ† Best Model

The best performing model is:

ğŸ‘‰ **XGBoost**

It provided the highest F1-score on the test set.

Saved files:
models/best_model.pkl
models/tfidf_vectorizer.pkl

---

## ğŸ”® How to Use the Prediction Script

You can predict whether a message is spam directly from the terminal:

### 1ï¸âƒ£ Run the script
python src/predict.py â€œCongratulations! Youâ€™ve won a free prizeâ€
### 2ï¸âƒ£ Output
SPAM

Example ham:
python src/predict.py â€œHey David, are we still meeting tonight?â€
HAM

---

## ğŸ“¦ Installation

Install dependencies:
pip install -r requirements.txt

---

## ğŸ“Œ Next Steps (Future Work)

- Deploy a **Streamlit web app**
- Add a **FastAPI REST API**
- Improve preprocessing (stemming, lemmatization)
- Train transformer-based models (BERT, DistilBERT)
- Handle dataset imbalance with SMOTE
- Add hyperparameter tuning (GridSearch)

---

## ğŸ‘¨â€ğŸ’» Author

Davidson ADRIEN â€” Data Scientist & Machine Learning Enthusiast  
Project created for educational and portfolio purposes.

---